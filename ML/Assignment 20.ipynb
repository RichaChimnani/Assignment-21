{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 20\n",
    "\n",
    "1. What are the three stages to build the hypothesis or model in machine learning?\n",
    "2. What is the standard approach to supervised learning?\n",
    "3. What is training set and test set?\n",
    "4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "5. How can you avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What are the three stages to build the hypothesis or model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis originates from the Greek work hupo (under) and thesis (placing). It means an idea made from limited evidence.\n",
    "It is a starting point for further investigation. The notion is simple yet powerful. We perform hypothesis testing intuitively\n",
    "every day. \n",
    "It is a 7-step process:\n",
    "\n",
    "1. Make Assumptions.\n",
    "2. Take an initial position.\n",
    "3. Determine the alternate position.\n",
    "4. Set acceptance criteria\n",
    "5. Conduct fact based tests.\n",
    "6. Evaluate results. Does the evaluation support the initial position? Are we confident that the result is not due to chance?\n",
    "7. Reach one of the following conclusion: Reject the original position in favor of alternate position or fail to reject the \n",
    "    initial position.\n",
    "\n",
    "The three stages to build the hypothesis or model in machine learning are as follows:\n",
    "\n",
    "i) Model Building: Choose the suitable algorithm for the model and train it according to the requirement of your problem.\n",
    "\n",
    "ii) Model testing: Check the accuracy of the model through the test data. \n",
    "\n",
    "iii) Applying the model: Make the required changes after testing and apply the final model which we have at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve a given problem of supervised learning, one has to perform the following steps:\n",
    "    \n",
    "1) Determine the type of training examples. Before doing anything else, the engineer should decide what kind of data is to \n",
    "   be used as an example. For instance, this might be a single handwritten character, an entire handwritten word, or an \n",
    "   entire line of handwriting.\n",
    "            \n",
    "2) Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of \n",
    "   input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\n",
    "\n",
    "3) Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly \n",
    "   on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains \n",
    "   a number of features that are descriptive of the object. The number of features should not be too large, because of the \n",
    "   curse of dimensionality; but should contain enough information to accurately predict the output.\n",
    "            \n",
    "4) Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose\n",
    "   to use support vector machines or decision trees.\n",
    "\n",
    "5) Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require \n",
    "   the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset \n",
    "    (called a  validation set) of the training set, or via cross-validation.\n",
    "\n",
    "6) Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting\n",
    "   function should be measured on a test set that is separate from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is training set and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine-learning algorithm is a mathematical model that learns to find patterns in the input that is fed to it. This input \n",
    "is referred to as training data. \n",
    "Once a machine learning algorithm learns the underlying patterns of the training data, it needs to be tested on fresh data \n",
    "(or test data) that it has never seen before, but which still belongs to the same distribution as the training data.\n",
    "Training set are distinct from Test set.\n",
    "\n",
    "Training Dataset: The sample of data used to fit the model.\n",
    "The actual dataset that we use to train the model (weights and biases in the case of Neural Network). The model sees and \n",
    "learns from this data.\n",
    "\n",
    "Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained\n",
    "(using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on \n",
    "many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is \n",
    "only released when the competition is about to close, and it is the result of the the model on the Test set that decides the\n",
    "winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally \n",
    "well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the \n",
    "real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning \n",
    "algorithm in order to improve robustness over a single model. \n",
    "\n",
    "Error in Ensemble Learning.\n",
    "The error emerging from any model can be broken down into three components mathematically. Following are these component :\n",
    "1. Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high \n",
    "bias error means we have a under-performing model which keeps on missing important trends.\n",
    "\n",
    "2.Variance on the other side quantifies how are the prediction made on same observation different from each other. A high \n",
    "variance model will over-fit on your training population and perform badly on any observation beyond training. \n",
    "Normally, as we increase the complexity of model, we will see a reduction in error due to lower bias in the model. However, \n",
    "this only happens till a particular point. As we continue to make our model more complex, we end up over-fitting our model \n",
    "and hence our model will start suffering from high variance.\n",
    "\n",
    "Some Commonly used Ensemble learning techniques\n",
    "\n",
    "1. Bagging In enemble: Bagging tries to implement similar learners on small sample populations and then takes a mean of all\n",
    "the predictions. In generalized bagging, we can use different learners on different population.  As we can expect this helps \n",
    "us to reduce the variance error.\n",
    "\n",
    "2.Boosting in ensemble: Boosting is an iterative technique which adjust the weight of an observation based on the last \n",
    "classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and \n",
    "vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes \n",
    "over fit on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How can you avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is overfitting?\n",
    "\n",
    "The word overfitting refers to a model that models the training data too well. Instead of learning the genral distribution of\n",
    "the data, the model learns the expected output for every data point. This is the same a memorizing the answers to a maths \n",
    "quizz instead of knowing the formulas. Because of this, the model cannot generalize. Everything is all good as long as we are\n",
    "in familiar territory, but as soon as we step outside, weâ€™re lost. The tricky part is that, at first glance, it may seem that\n",
    "our model is performing well because it has a very small error on the training data. However, as soon as we ask it to predict\n",
    "new data points, it will fail.\n",
    "\n",
    "How to detect overfitting.\n",
    "\n",
    "As stated above, overfitting is characterized by the inability of the model to generalize. To test this ability, a simple \n",
    "method consists in splitting the dataset into two parts: the training set and the test set. When selecting models, we might \n",
    "want to split the dataset in three.\n",
    "1. The training set represents about 80% of the available data, and is used to train the model.\n",
    "2. The test set consists of the remaining 20% of the dataset, and is used to test the accuracy of the model on data it has \n",
    "never seen before.\n",
    "With this split we can check the performance of the model on each set to gain insight on how the training process is going, \n",
    "and spot overfitting when it happens. This table shows the different cases.\n",
    "\n",
    "\n",
    "# Steps for reducing overfitting:\n",
    "\n",
    "i) Add more data. \n",
    "ii) Use data augmentation. \n",
    "iii) Use architectures that generalize well. \n",
    "iv) Add regularization (mostly dropout, L1/L2 regularization are also possible) \n",
    "v) Reduce architecture complexity.\n",
    "\n",
    "Following are the commonly used methodologies :\n",
    "\n",
    "1.Cross-Validation : Cross Validation in its simplest form is a one round validation, where we leave one sample as in-time \n",
    "validation and rest for training the model. But for keeping lower variance a higher fold cross validation is preferred.\n",
    "2.Early Stopping : Early stopping rules provide guidance as to how many iterations can be run before the learner begins to \n",
    "over-fit.\n",
    "3.Pruning : Pruning is used extensively while building CART models. It simply removes the nodes which add little predictive \n",
    "power for the problem in hand.\n",
    "4.Regularization : This is the technique we are going to discuss in more details. Simply put, it introduces a cost term for \n",
    "bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero \n",
    "and hence reduce cost term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
